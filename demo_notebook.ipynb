{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51c95d1-b447-4f1b-9571-cf597ca93ef4",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  text-align: center; color:#8735fb; font-size:22pt\"> **HPO Benchmarking with RAPIDS and Dask** </span>\n",
    "\n",
    "Hyper-Parameter Optimization (HPO) helps to find the best version of a model by exploring the space of possible configurations. While generally desirable, this search is computationally expensive and time-consuming.\n",
    "\n",
    "In the notebook demo below, we compare benchmarking results to show how RAPIDS can accelerate HPO tuning jobs relative to CPU.\n",
    "\n",
    "For instance, we find a x speedup in wall clock time (6 hours vs 3+ days) and a x reduction in cost when comparing between GPU and CPU EC2 instances on 100 XGBoost HPO trials using 10 parallel workers on 10 years of the Airline Dataset.\n",
    "\n",
    "For more check out our AWS blog(link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d178f5-fc5d-471e-9898-544e5fdbc271",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  color:#8735fb; font-size:22pt\"> **Preamble** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c311bd4-76a1-4ee7-8841-5d44ca052566",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 1.1 Create EC2 instance </span>\n",
    "\n",
    "Create a new Instance with GPUs, the NVIDIA Driver and the NVIDIA Container Runtime.\n",
    "\n",
    "Amazon maintains an [Amazon Machine Image (AMI)](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-gpu-tensorflow-2-12-amazon-linux-2/) that pre-installs NVIDIA drivers and container runtimes, we recommend using this image as the starting point.\n",
    "\n",
    "1. **Open the EC2 Dashboard**.\n",
    "\n",
    "2. **Select Launch Instance**.\n",
    "\n",
    "3. In the AMI selection box under **\"Amazon Machine Image (AMI)\"**, select the [Deep Learning AMI GPU TensorFlow or PyTorch](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html) \n",
    "<img src='img/launch-ec2.png'>\n",
    "4) Choose **RAPIDS compatible instance type**, must be Pascal or higher (e.g. \"p3.8xlarge\")\n",
    "\n",
    "6) Select your SSH key-pair (create one if you havenâ€™t already).\n",
    "\n",
    "7) Under network settings create/choose existing security group that allows SSH access on port 22\n",
    "\n",
    "8) Review and **Launch**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd308616-15ff-4244-983b-c3a531870b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 1.2 Connect to the instance </span>\n",
    "\n",
    "Next we need to connect to the instance.\n",
    "\n",
    "1. Open the EC2 Dashboard.\n",
    "\n",
    "2. Locate your VM and note the Public IP Address.\n",
    "\n",
    "3. In your terminal run `ssh -i <key-pair-name > ec2-user@<ip address>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe66ea-f93f-4954-9a6f-1b9e850cc387",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **2. ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3fbaf-754b-45a6-959c-7163edfe5c4f",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.1 - Dataset </span>\n",
    "\n",
    "The data source for this workflow is 3 years of the [Airline On-Time Statistics](https://www.transtats.bts.gov/ONTIME/) dataset from the US Bureau of Transportation.\n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* Locations and distance ( Origin, Dest, Distance )\n",
    "* Airline / carrier ( Reporting_Airline )\n",
    "* Scheduled departure and arrival times ( CRSDepTime and CRSArrTime )\n",
    "* Actual departure and arrival times ( DpTime and ArrTime )\n",
    "* Difference between scheduled & actual times ( ArrDelay and DepDelay )\n",
    "* Binary encoded version of late, aka our target variable ( ArrDelay15 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b5a5b-87cf-4ff6-84f2-4485c7c0470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd10f79-23b2-440e-89b4-f6e4a5c9f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOWNLOAD THE DATASET\n",
    "!aws s3 cp --recursive s3://sagemaker-rapids-hpo-us-west-2/3_year/ ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1759de-98af-4628-a79b-a236a2dee5a2",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.2 - LocalCUDACluster </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533be0b1-0d5e-46b3-9ff1-dd71751fe68f",
   "metadata": {},
   "source": [
    "To maximize on efficiency, we launch a `LocalCUDACluster` that utilizes GPUs for distributed computing. Then connect a Dask Client to submit and manage computations on the cluster. Refer to this (link) for more information on how to achieve this.\n",
    "\n",
    "Submit dataset to the Dask client, instructing Dask to store the dataset in memory  at all times. This can improve performance by avoiding unnecessary data transfers during the hpo process. \n",
    "\n",
    "    with LocalCUDACluster() as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            dataset = ingest_data()\n",
    "            client.persist(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d57ba2-df8a-4757-a0df-44b3cd73b75c",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.3 - Python ML Workflow </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddb8fd-2ac8-4257-8f7f-47f71c6d76c2",
   "metadata": {},
   "source": [
    "In order to work with RAPIDS container, the entrypoint logic should parse model-type parameter (manually supplied at script run), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyperparameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcb2c4-526c-466e-88b6-00c202f494c2",
   "metadata": {},
   "source": [
    "`Optuna` is a hyperparameter optimization library in Python. We create an Optuna `study object` that provides a framework to define the search space, objective function, and optimization algorith for the hpo  process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2d4ec4-b6e4-4546-8177-505f2739c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skirui/home/skirui/tco_hpo_gpu_cpu_perf_benchmark/code\n"
     ]
    }
   ],
   "source": [
    "%cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f715efe-0e23-4b12-aac3-49e0e3b575b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  hpo_cpu.py  hpo_gpu.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89edfea-ca14-4d26-94c6-0ef8eaf02d77",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **3. Build RAPIDS Container** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230601ec-74c8-4299-85c1-bf416192fd43",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 3.1 - Containerize and Build </span>\n",
    "\n",
    "Build from the latest Rapids container and install other necessary dependencies. Your dockerfile should look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd07ad98-9d9d-4f9f-ba4e-1b0a1646cc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# FROM nvcr.io/nvidia/rapidsai/rapidsai-core:23.06-cuda11.5-runtime-ubuntu20.04-py3.10\n",
      "\n",
      "FROM rapidsai/rapidsai:23.06-cuda11.5-runtime-ubuntu20.04-py3.10\n",
      "\n",
      "#FROM rapidsai/rapidsai:23.06-cuda11.8-runtime-ubuntu22.04-py3.10\n",
      "\n",
      "RUN mamba install -y -n rapids optuna\n"
     ]
    }
   ],
   "source": [
    "cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b72cbf8-1f93-4e63-a767-d87210697eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                              TAG                                         IMAGE ID       CREATED       SIZE\n",
      "rapidsai/rapidsai                       23.06-cuda11.8-runtime-ubuntu22.04-py3.10   027b84613f4f   2 weeks ago   17.3GB\n",
      "rapidsai/rapidsai                       23.06-cuda11.5-runtime-ubuntu20.04-py3.10   4db4b31a94fc   2 weeks ago   16.1GB\n",
      "nvcr.io/nvidia/rapidsai/rapidsai-core   23.06-cuda11.5-runtime-ubuntu20.04-py3.10   597389c5fd96   2 weeks ago   16GB\n",
      "<none>                                  <none>                                      d5a111ce0d10   3 weeks ago   4.18GB\n",
      "rapidsai/rapidsai-core                  23.06-cuda11.5-runtime-ubuntu20.04-py3.10   8a9b2eee5b32   3 weeks ago   16GB\n",
      "databricksruntime/gpu-conda             cuda11                                      cd895161062c   2 years ago   4.18GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9919b0eb-70f6-43cb-8d64-5bc9e3b2ace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 11 21:27:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:15:00.0 Off |                  Off |\n",
      "| 34%   41C    P8    24W / 260W |     10MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:2D:00.0  On |                  Off |\n",
      "| 33%   47C    P8    30W / 260W |    706MiB / 48593MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1626      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      3419      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      1626      G   /usr/lib/xorg/Xorg                 71MiB |\n",
      "|    1   N/A  N/A      1695      G   /usr/bin/gnome-shell               90MiB |\n",
      "|    1   N/A  N/A      3419      G   /usr/lib/xorg/Xorg                279MiB |\n",
      "|    1   N/A  N/A      3550      G   /usr/bin/gnome-shell              180MiB |\n",
      "|    1   N/A  N/A     24204      G   ...2gtk-4.0/WebKitWebProcess       80MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0ccfd-dc68-4c1a-8a31-e74a5d02b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  15.87kB\n",
      "Step 1/2 : FROM rapidsai/rapidsai:23.06-cuda11.5-runtime-ubuntu20.04-py3.10\n",
      " ---> 4db4b31a94fc\n",
      "Step 2/2 : RUN mamba install -y -n rapids optuna\n",
      " ---> Running in b73155fe6a06\n",
      "Transaction\n",
      "\n",
      "  Prefix: /opt/conda/envs/rapids\n",
      "\n",
      "  Updating specs:\n",
      "\n",
      "   - optuna\n",
      "   - ca-certificates\n",
      "   - certifi\n",
      "   - openssl\n",
      "\n",
      "\n",
      "  Package       Version  Build            Channel                   Size\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Install:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  + alembic      1.11.1  pyhd8ed1ab_0     conda-forge/noarch       154kB\n",
      "  + cmaes         0.9.1  pyhd8ed1ab_0     conda-forge/noarch        21kB\n",
      "  + colorlog      6.7.0  py310hff52083_1  conda-forge/linux-64      18kB\n",
      "  + greenlet      2.0.2  py310hc6cd4ac_1  conda-forge/linux-64     191kB\n",
      "  + mako          1.2.4  pyhd8ed1ab_0     conda-forge/noarch        63kB\n",
      "  + optuna        3.2.0  pyhd8ed1ab_0     conda-forge/noarch       229kB\n",
      "  + sqlalchemy   2.0.18  py310h2372a71_0  conda-forge/linux-64       3MB\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 7 packages\n",
      "\n",
      "  Total download: 3MB\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "\n",
      "Looking for: ['optuna']\n",
      "\n",
      "\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!docker build -t rapids-tco-benchmark:v23.06 -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1959fa-f70d-4940-8204-1946acd0e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f881ee-ce7f-4810-a6b7-fa8aa72d91f3",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **4. Run HPO** </span>\n",
    "\n",
    "-- should cover the code snippets and what the code does \n",
    "* define metric\n",
    "* define tuner\n",
    "* run\n",
    "* results and summary\n",
    "\n",
    "\n",
    "- Start a tmux session\n",
    "- Run container with mount -v option , expose nvidia gpus and jupyter\n",
    "- Run python scripts inside container, passing \"model type\" as argument and output benchmark results into a txt file\n",
    "- Wait until experiments finish running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fa632-2e20-49a1-bbf6-932def2228bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b027b36d-a85b-4ada-b137-880231c09b9e",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> 5. Cleanup </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "rapids-23.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
