{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51c95d1-b447-4f1b-9571-cf597ca93ef4",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  text-align: center; color:#8735fb; font-size:22pt\"> **HPO Benchmarking with RAPIDS and Dask** </span>\n",
    "\n",
    "Hyper-Parameter Optimization (HPO) helps to find the best version of a model by exploring the space of possible configurations. While generally desirable, this search is computationally expensive and time-consuming.\n",
    "\n",
    "In the notebook demo below, we compare benchmarking results to show how RAPIDS can accelerate HPO tuning jobs relative to CPU.\n",
    "\n",
    "For instance, we find a x speedup in wall clock time (6 hours vs 3+ days) and a x reduction in cost when comparing between GPU and CPU EC2 instances on 100 XGBoost HPO trials using 10 parallel workers on 10 years of the Airline Dataset.\n",
    "\n",
    "For more check out our AWS blog(link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d178f5-fc5d-471e-9898-544e5fdbc271",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  color:#8735fb; font-size:22pt\"> **Preamble** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c311bd4-76a1-4ee7-8841-5d44ca052566",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 1.1 Create EC2 instance </span>\n",
    "\n",
    "Create a new Instance with GPUs, the NVIDIA Driver and the NVIDIA Container Runtime.\n",
    "\n",
    "Amazon maintains an [Amazon Machine Image (AMI)](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-gpu-tensorflow-2-12-amazon-linux-2/) that pre-installs NVIDIA drivers and container runtimes, we recommend using this image as the starting point.\n",
    "\n",
    "1. **Open the EC2 Dashboard**.\n",
    "\n",
    "2. **Select Launch Instance**.\n",
    "\n",
    "3. In the AMI selection box under **\"Amazon Machine Image (AMI)\"**, select the [Deep Learning AMI GPU TensorFlow or PyTorch](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html) \n",
    "<img src='img/launch-ec2.png'>\n",
    "4) Choose **RAPIDS compatible instance type**, must be Pascal or higher (e.g. \"p3.8xlarge\")\n",
    "\n",
    "6) Select your SSH key-pair (create one if you havenâ€™t already).\n",
    "\n",
    "7) Under network settings create/choose existing security group that allows SSH access on port 22\n",
    "\n",
    "8) Review and **Launch**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd308616-15ff-4244-983b-c3a531870b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 1.2 Connect to the instance </span>\n",
    "\n",
    "Next we need to connect to the instance.\n",
    "\n",
    "1. Open the EC2 Dashboard.\n",
    "\n",
    "2. Locate your VM and note the Public IP Address.\n",
    "\n",
    "3. In your terminal run `ssh -i <key-pair-name > ec2-user@<ip address>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe66ea-f93f-4954-9a6f-1b9e850cc387",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **2. ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3fbaf-754b-45a6-959c-7163edfe5c4f",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.1 - Dataset </span>\n",
    "\n",
    "The data source for this workflow is 3 years of the [Airline On-Time Statistics](https://www.transtats.bts.gov/ONTIME/) dataset from the US Bureau of Transportation.\n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* Locations and distance ( Origin, Dest, Distance )\n",
    "* Airline / carrier ( Reporting_Airline )\n",
    "* Scheduled departure and arrival times ( CRSDepTime and CRSArrTime )\n",
    "* Actual departure and arrival times ( DpTime and ArrTime )\n",
    "* Difference between scheduled & actual times ( ArrDelay and DepDelay )\n",
    "* Binary encoded version of late, aka our target variable ( ArrDelay15 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b5a5b-87cf-4ff6-84f2-4485c7c0470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd10f79-23b2-440e-89b4-f6e4a5c9f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOWNLOAD THE DATASET\n",
    "!aws s3 cp --recursive s3://sagemaker-rapids-hpo-us-west-2/3_year/ ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1759de-98af-4628-a79b-a236a2dee5a2",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.2 - Local Cluster </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533be0b1-0d5e-46b3-9ff1-dd71751fe68f",
   "metadata": {},
   "source": [
    "To maximize on efficiency, we launch a `LocalCUDACluster` that utilizes GPUs for distributed computing. Then connect a Dask Client to submit and manage computations on the cluster. Refer to this (link) for more information on how to achieve this.\n",
    "\n",
    "Submit dataset to the Dask client, instructing Dask to store the dataset in memory  at all times. This can improve performance by avoiding unnecessary data transfers during the hpo process. \n",
    "\n",
    "    with LocalCUDACluster() as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            dataset = ingest_data()\n",
    "            client.persist(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d57ba2-df8a-4757-a0df-44b3cd73b75c",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.3 - Python ML Workflow </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddb8fd-2ac8-4257-8f7f-47f71c6d76c2",
   "metadata": {},
   "source": [
    "In order to work with RAPIDS container, the entrypoint logic should parse model-type parameter (manually supplied at script run), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyperparameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcb2c4-526c-466e-88b6-00c202f494c2",
   "metadata": {},
   "source": [
    "`Optuna` is a hyperparameter optimization library in Python. We create an Optuna `study object` that provides a framework to define the search space, objective function, and optimization algorith for the hpo  process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d4ec4-b6e4-4546-8177-505f2739c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f715efe-0e23-4b12-aac3-49e0e3b575b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89edfea-ca14-4d26-94c6-0ef8eaf02d77",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> **3. Build RAPIDS Container** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919b0eb-70f6-43cb-8d64-5bc9e3b2ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07ad98-9d9d-4f9f-ba4e-1b0a1646cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72cbf8-1f93-4e63-a767-d87210697eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0ccfd-dc68-4c1a-8a31-e74a5d02b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t rapids-tco-benchmark:v23.06 -f ./code/Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1959fa-f70d-4940-8204-1946acd0e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b8b7f-1634-42b8-81e8-2605195d2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tmux exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1b2c6-5681-4852-ae10-77fbc10ba3d3",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> Run Container </span>\n",
    "\n",
    "Start the container making sure to mount your code with the `-v flag` option. Be sure to expose the JupyterLab serve to your host machine via ports 8786-8888."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b59cf6-963e-4a08-85f7-267b83a99cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --gpus all -p 8888:8888 -p 8787:8787 -p 8786:8786 -v \\\n",
    "                    /home/ec2-user/tco_hpo_gpu_cpu_perf_benchmark:/rapids/notebooks/host \\\n",
    "                            rapids-tco-benchmark:v23.06 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f881ee-ce7f-4810-a6b7-fa8aa72d91f3",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **4. Run HPO** </span>\n",
    "\n",
    "Navigate to the host directory inside the container and run the python script with the following command : \n",
    "\n",
    "    python ./hpo.py --model-type \"XGBoost\" > skirui_xgboost_cpu.txt 2>&1\n",
    "\n",
    "\n",
    "Let's take a deeper dive into the training script for a better understanding of the \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9ae6e-55c7-4416-8437-71e5aaee0790",
   "metadata": {},
   "source": [
    "* define metric\n",
    "* define tuner\n",
    "* run\n",
    "* results and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027b36d-a85b-4ada-b137-880231c09b9e",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> 5. Cleanup </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d8ecf-d022-477b-ab8f-8c793f5106c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./hpo.py --model-type \"XGBoost\" > ./code/xgboost_cpu.txt 2>&1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "rapids-23.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
